Presentation 2016-10-27
Movie script





    NARRATOR:
These are the Chalmers groups of the WASP Automonous Systems course. Today, we'll see how far they've come.

We see a few short clips, accompanied by overlaid text read aloud by the narrator:

Slow pan over a text editor with scheduler.py open. Overlaid text: "Autonomous planning and scheduling"

Turtlebot moving. Overlaid text: "Ground robot moves from A to B, avoiding obstacles"

Drone moving. Overlaid text: "Drone moves from A to B"

Video feed of slam, preferably from drone while in flight. Overlaid text: "Drone uses SLAM."

    TOMASZ:
Our task is to deliver crates to disaster victims. We have a drone that can pick up the crates, but it can't deliver it far. We have a ground robot that can drive the crates far, but can't pick them up!

A close-up of a computer running the scheduler, getting info back from dummy robots.
    NARRATOR, VO:
The solution lies in cooperation. Their system has a centralised node which finds the optimal plan for the available robots, and directs them in real time.

    TOMASZ:
The robots move independently of one another, each one depends only on the central scheduler. We call this architecture, of course, Skynet.

    NARRATOR:
Before they show us a full scenario, let's go through the individual parts.

Splash screen: "The ground robots"

    TOMMY:
This here turtlebot represents the unmanned ground vehicle. It has an internal map, which places the origin at where it was when it was booted up.

    NARRATOR:
Using this coordinate system, Tommy can set a destination, and the turtlebot will go there, avoiding obstacles along the way.

Splash screen: "The aerial drones"

    IVO (or SOME DRONE GUY):
This is an ARDrone. It can fly.

The drone flies around for a bit.

    TOMASZ:
We are truly living in the future.

    IVO:
So we use PID controllers for pitch and roll, with the objective of keeping the drone still. As long as we have a reliable source telling us the drones position, we can control it hover in place.

    NARRATOR:
And let's meet the guy tasked with providing that reliable source.

A door opens, entering SAMUEL's room. He is inside, booting up the SLAM feature for a camera he's holding, connected to the computer via cable. He holds up one hand indicating that we should wait. He is otherwise fully focussed on the computer screen.

    SAMUEL, VO: (CUT to SAMUEL talking a few words into the clip)
We use camera-based SLAM, so the drone controller gets a feed of the drone's position based on what it can see. It loses track if it moves too fast, but usually it's fine.

CUT partway through, SAMUEL continues as VO. We see someone holding a camera, and a monitor showing that it is running SLAM. The operator shakes the SLAM camera.

A drone flies to a point and stops.

    NARRATOR:
Based on the positioning data from SLAM, the drone crew can tell the drone where to go.

    IVO:
What we do is we feed the destination into the controller as essentially an error signal, so when the drone tries to correct the position, it moves to where we want it to be.

People working together, testing drones and robots.

    NARRATOR:
The marriage between SLAM and PID control to complete the task, actually hints to a broader collaboration effort. The Chalmers team actually consists of two teams, who joined forces a few weeks into the project.

    TOMASZ:
It was partly for logistical reasons; we had a limited amount of robots, and had to split them over two campuses. But it was also because after all, it would feel stupid to do the same work twice.

    NARRATOR:
And so they didn't. When they decided to join forces, team 1 already had a planner ready to go. Team 2, in turn, brought their work on the turtlebot to the table.

Lingering stills of the two teams, labeled "CTH-1" and "CTH-2". Then a slide showing 'Planning' in the left-hand column, 'Turtlebot' in the right-hand column, and 'Drones', '
SLAM' and 'Integration'

    NARRATOR:
And now it's time to show a live run of the search-and-rescue operation.

We see the robots in a large room, performing the main sequence, with narration at times. An overlay depicts the architecture and the two plans, updated live.

Someone starts the planner.

    NARRATOR:
The planner reads the input problem, and creates a plan. Then, it waits for the two robots to connect.

Someone starts the turtlebot. It is positioned right beneath the april tag.

    NARRATOR:
The team starts the ground robot. It is positioned precisely beneath the april tag that will serve as the point of origin in the global coordinate system.

    NARRATOR:
The ground robot immediately receives its first order, to move to the depot. But when it gets there, it will have to wait. It can't pick up the crate on its own. So let's start the drone.

Someone starts the drone. It lifts from the ground; it is positioned to see the april tag.

    NARRATOR:
After the SLAM initialisation, the drone reports to the scheduler that it's ready for its first command. It gets told to go load a crate on the ground robot.

    NARRATOR:
When the drone has finished loading the crate, the ground robot is cleared for moving. It sets off towards the victim.

    NARRATOR: (as the turtle moves to the victim)
This is all done autonomously â€“ the operators have not touched anything since they started the drone.

    NARRATOR:
When the ground robot arrives, the victim can get the crate.

After the crate is delivered, the text "Plan finished" show up on screen. Fade to black.

    TOMASZ:
Our system could theoretically handle more than one robot of each type, but there are of course budgetary and logistical constraints. Also, if we had several aerial drones they would probably fly into each other.

Text over black background, appearing line by line:
Future work:
- Representing crates and victims IRL
- Mapping area using robots
    NARRATOR:
There are other parts of the challenge the Chalmers team has not yet solved. Most importantly, they currently define the locations of crates and victims in a specifications file. Their final solution will hopefully be able to send the drones out to scout for victims, create an initial map, and send that to the planner.

Let's hope they, and their drones, can deliver in time.













